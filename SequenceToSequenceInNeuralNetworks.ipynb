{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+jUvj+9t4HiTacaQ2SHuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuadV/ImplementingPapers/blob/main/SequenceToSequenceInNeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38FlHHZi1jyY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter # to print to tensorboard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlAT8ZKvOBqJ",
        "outputId": "93eb12a8-4856-4ffe-bb41-b8d88b069216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "! python -m spacy download de"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.2.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=f4adbd5f21d88ff4c0b1129314a037506e9b1ec91399516e634c0ca0be13b7c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hjqooj2p/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU_Xf8c5XHMe"
      },
      "source": [
        "spacy_ger = spacy.load('de')\n",
        "spacy_eng = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvSww3vTNkJj"
      },
      "source": [
        "def tokenizer_ger(text):\n",
        "  \"\"\" Hello my name -> ['Hello', 'my', 'name']\"\"\"\n",
        "  return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "\n",
        "def tokenizer_eng(text):\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaFf1ipaOj8S"
      },
      "source": [
        "german = Field(tokenize=tokenizer_ger, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "english = Field(tokenize=tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C7yQeQUPLlc",
        "outputId": "d2051c1b-a6c6-45b6-dcef-b558c466b820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))\n",
        "\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rtraining.tar.gz:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 5.11MB/s]\n",
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 1.35MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n",
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 1.34MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI97muI0R7JV"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size= hidden_size\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x shape = (seq_length, N) # seq_length of words in N batches\n",
        "\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    # embedding shape: (seq_len, N, embedding_size)\n",
        "    output, (hidden, cell) = self.rnn(embedding) \n",
        "    return hidden, cell\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-7tPMuyR_pi"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size,\n",
        "               num_layers, dropout): # input_size=output_size coz it will be prob of word in vocab 10000\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "    # shape of x: (N) but we want (1, N) # 1 word at a time in N batches\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    # embedding shape: (1, N, embedding_size)\n",
        "    outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "    # shape of outputs: (1, N, hidden_size)\n",
        "\n",
        "    predictions = self.fc(outputs)\n",
        "    # shape of predictions: (1, N, length_of_vocab)\n",
        "    predictions = predictions.squeeze(0)  # add ouput from decoder one step at a time. hence adding is simplified in this shape\n",
        "    return predictions, hidden, cell"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UadE0VBhSCR0"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5): # sometimes the prediction, sometimes the actual word when training\n",
        "    batch_size = source.shape[1]\n",
        "    # source: (trg_len, N)\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    hidden, cell = self.encoder(source)\n",
        "    # grab start token\n",
        "    x = target[0]\n",
        "\n",
        "    for t in range(target_len):\n",
        "      output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "      outputs[t] = output\n",
        "      # output: (N, eng_vocab_size) - argmax along 1st dimension to get the best guess of word perdicted\n",
        "      best_guess = output.argmax(1)\n",
        "\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "    return outputs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vryTxg6SatN8",
        "outputId": "a442803f-f517-40dd-8e15-3ad15f96b607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training\n",
        "\n",
        "# training hyperparameters\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "# model hyperparameters\n",
        "load_model = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size_encoder = len(german.vocab)\n",
        "input_size_decoder = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "num_layers = 2\n",
        "\n",
        "# Tensorboard\n",
        "writer = SummaryWriter(f'runs/loss_plot')\n",
        "step = 0\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key = lambda x: len(x.src), # sorts examples with similar length in batch. this saves on compute\n",
        "    device = device\n",
        ")\n",
        "\n",
        "encoder_net = Encoder(input_size=input_size_encoder, embedding_size=encoder_embedding_size, \n",
        "                      hidden_size=hidden_size, num_layers=num_layers, dropout=enc_dropout)\n",
        "decoder_net = Decoder(input_size=input_size_decoder, embedding_size=decoder_embedding_size, \n",
        "                      hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, dropout=dec_dropout)\n",
        "model = Seq2Seq(encoder=encoder_net, decoder=decoder_net).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx) # ignore padding index\n",
        "\n",
        "# if load_model:\n",
        "#   load_checkpoint(torch.load())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f'Epoch {epoch} / {num_epochs}')\n",
        "\n",
        "  checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "  # save checkpoint here\n",
        "\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    inp_data = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "\n",
        "    output = model(inp_data, target)\n",
        "    # output shape: (trg_len, batch_size, output_dim)\n",
        "\n",
        "    output = output[1:].reshape(-1, output.shape[2]) # keep vocab lengt and combine all other dimensions\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "    optimizer.step()\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 / 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re-lJmJTAVlh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}